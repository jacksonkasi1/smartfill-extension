// ** import types
import type { AIFormData, FormField } from '@/types/extension'
import type { LLMProvider } from './constants'

// ** import constants
import { PROVIDERS, DEFAULT_PROVIDER, DEFAULT_MODELS, LLM_GENERATION_CONFIG } from './constants'

// ** import utils
import { buildPrompt, type ModelMetadata } from './prompt'
import { parseAIResponse } from './parser'

// ** Provider Settings Interface
interface ProviderSettings {
  provider: LLMProvider
  model: string
  apiKey: string
}

/**
 * Get provider settings from storage
 */
async function getProviderSettings(): Promise<ProviderSettings> {
  const result = await chrome.storage.sync.get([
    'llmProvider',
    'llmModel',
    'llmApiKeys'
  ])

  // Determine provider (default to gemini)
  const provider: LLMProvider = result.llmProvider || DEFAULT_PROVIDER

  // Get API keys
  const apiKeys = result.llmApiKeys || {}

  // Get API key for selected provider
  const apiKey = apiKeys[provider]
  if (!apiKey) {
    throw new Error(`No API key found for ${PROVIDERS[provider].name}. Please configure it in settings.`)
  }

  // Get model (use default if not set)
  const model = result.llmModel || DEFAULT_MODELS[provider]

  return { provider, model, apiKey }
}

/**
 * Call Gemini API
 */
async function callGeminiAPI(prompt: string, model: string, apiKey: string): Promise<string> {
  const url = `${PROVIDERS.gemini.baseUrl}/${model}:generateContent?key=${apiKey}`

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        parts: [{
          text: prompt
        }]
      }]
    })
  })

  if (!response.ok) {
    let errorMessage = `Gemini API error: ${response.status} ${response.statusText}`
    try {
      const errorData = await response.json()
      if (errorData.error?.message) {
        errorMessage = errorData.error.message
      }
    } catch {
      // Keep original error message if JSON parsing fails
    }
    throw new Error(errorMessage)
  }

  const data = await response.json()
  const generatedText = data.candidates?.[0]?.content?.parts?.[0]?.text

  if (!generatedText) {
    throw new Error('No content generated by Gemini AI')
  }

  return generatedText
}

/**
 * Call Groq API (OpenAI-compatible)
 */
async function callGroqAPI(prompt: string, model: string, apiKey: string): Promise<string> {
  const url = `${PROVIDERS.groq.baseUrl}/chat/completions`

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: model,
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: LLM_GENERATION_CONFIG.temperature,
      max_tokens: LLM_GENERATION_CONFIG.maxTokens
    })
  })

  if (!response.ok) {
    let errorMessage = `Groq API error: ${response.status} ${response.statusText}`
    try {
      const errorData = await response.json()
      if (errorData.error?.message) {
        errorMessage = errorData.error.message
      }
    } catch {
      // Keep original error message if JSON parsing fails
    }
    throw new Error(errorMessage)
  }

  const data = await response.json()
  const generatedText = data.choices?.[0]?.message?.content

  if (!generatedText) {
    throw new Error('No content generated by Groq AI')
  }

  return generatedText
}

/**
 * Call OpenRouter API (OpenAI-compatible)
 */
async function callOpenRouterAPI(prompt: string, model: string, apiKey: string): Promise<string> {
  const url = `${PROVIDERS.openrouter.baseUrl}/chat/completions`

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
      'HTTP-Referer': 'https://github.com/jacksonkasi1/smartfill-extension',
      'X-Title': 'SmartFill Extension'
    },
    body: JSON.stringify({
      model: model,
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: LLM_GENERATION_CONFIG.temperature,
      max_tokens: LLM_GENERATION_CONFIG.maxTokens
    })
  })

  if (!response.ok) {
    let errorMessage = `OpenRouter API error: ${response.status} ${response.statusText}`
    try {
      const errorData = await response.json()
      if (errorData.error?.message) {
        errorMessage = errorData.error.message
      }
    } catch {
      // Keep original error message if JSON parsing fails
    }
    throw new Error(errorMessage)
  }

  const data = await response.json()
  const generatedText = data.choices?.[0]?.message?.content

  if (!generatedText) {
    throw new Error('No content generated by OpenRouter AI')
  }

  return generatedText
}

/**
 * Main function to generate form data using the configured LLM provider
 */
export async function generateFormData(fields: FormField[], customPrompt?: string): Promise<AIFormData> {
  // Get provider settings
  const { provider, model, apiKey } = await getProviderSettings()

  // Determine if model is custom or recommended
  const isRecommended = PROVIDERS[provider].models.some(m => m.id === model)
  const modelType: 'recommended' | 'custom' = isRecommended ? 'recommended' : 'custom'

  // Build model metadata for prompt
  const modelMetadata: ModelMetadata = {
    provider: PROVIDERS[provider].name,
    modelId: model,
    modelType
  }

  // Build prompt with model metadata
  const prompt = buildPrompt(fields, customPrompt, modelMetadata)

  // Call appropriate API based on provider
  let generatedText: string

  switch (provider) {
    case 'gemini':
      generatedText = await callGeminiAPI(prompt, model, apiKey)
      break

    case 'groq':
      generatedText = await callGroqAPI(prompt, model, apiKey)
      break

    case 'openrouter':
      generatedText = await callOpenRouterAPI(prompt, model, apiKey)
      break

    default:
      throw new Error(`Unsupported provider: ${provider}`)
  }

  // Parse and return the response
  return parseAIResponse(generatedText, fields)
}